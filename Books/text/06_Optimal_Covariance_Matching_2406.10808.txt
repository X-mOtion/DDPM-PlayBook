

=== PAGE 1 ===
Published as a conference paper at ICLR 2025
IMPROVING PROBABILISTIC DIFFUSION MODELS WITH
OPTIMAL DIAGONAL COVARIANCE MATCHING
Zijing Ou1∗, Mingtian Zhang 2∗, Andi Zhang 3, Tim Z. Xiao 456, Yingzhen Li 1, David Barber 2
1Imperial College London, 2University College London, 3University of Manchester,
4Max Planck Institute for Intelligent Systems, Tubingen 5University of Tubingen, 6IMPRS-IS.
z.ou22@imperial.ac.uk m.zhang@cs.ucl.ac.uk
ABSTRACT
The probabilistic diffusion model has become highly effective across various do-
mains. Typically, sampling from a diffusion model involves using a denoising
distribution characterized by a Gaussian with a learned mean and either fixed or
learned covariances. In this paper, we leverage the recently proposed covariance
moment matching technique and introduce a novel method for learning the diago-
nal covariance. Unlike traditional data-driven diagonal covariance approximation
approaches, our method involves directly regressing the optimal diagonal analytic
covariance using a new, unbiased objective named Optimal Covariance Matching
(OCM). This approach can significantly reduce the approximation error in covari-
ance prediction. We demonstrate how our method can substantially enhance the
sampling efficiency, recall rate and likelihood of commonly used diffusion models.
1 I NTRODUCTION
Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song & Ermon, 2019) have achieved
remarkable success in modeling complex data across various domains (Rombach et al., 2022; Li et al.,
2022; Poole et al., 2022; Ho et al., 2022; Hoogeboom et al., 2022; Liu et al., 2023). A conventional
diffusion model operates in two stages: a forward noising process, indexed by t ∈ [1, T], which
progressively corrupts the data distribution into a Gaussian distribution via Gaussian convolution,
and a reverse denoising process, which generates images by gradually transforming Gaussian noise
back into coherent data samples. In traditional diffusion models, the generation process typically
predicts only the mean of the denoising distribution while using a fixed, pre-defined variance (Ho
et al., 2020). This approach often requires a very large number of steps, T , to produce high-quality,
diverse samples or to achieve reasonable model likelihoods, leading to inefficiencies during inference.
To address this inefficiency, several works have proposed estimating the diagonal covariance of
denoising distributions rather than relying on pre-defined variance values. For instance, Bao et al.
(2022b) introduces an analytical form of isotropic, state-independent covariance that can be estimated
from the data. This analytical solution achieves an optimal form under the constraints of isotropy
and state independence. A more flexible approach involves learning a state-dependent diagonal
covariance. Nichol & Dhariwal (2021) propose learning this form by optimizing the variational lower
bound (VLB). Bao et al. (2022a) explore learning a state-dependent diagonal covariance directly from
data, also examining its analytical form. These methods have demonstrated improved image quality
with fewer denoising steps. Learning the covariance through VLB optimization (Nichol & Dhariwal,
2021) has become a widely adopted strategy in state-of-the-art image and video generative models.
Building on this line of research, the goal of our paper is to develop an improved denoising covariance
strategy that enhances both the generation quality and likelihood evaluation while reducing the
number of total time steps. Recently, Zhang et al. (2024b) derived the optimal state-dependent full
covariance for the denoising distribution. While this method offers greater flexibility than state-
dependent diagonal covariance, it requires O(D2) storage for the Hessian matrix and O(D) network
evaluations per denoising step, where D is the data dimension. This makes it impractical for high-
dimensional applications, such as image generation. To address this limitation, we propose a novel,
∗Equal contribution. Code is available at: https://github.com/J-zin/OCM_DPM.
1
arXiv:2406.10808v4  [cs.LG]  19 Feb 2025


=== PAGE 2 ===
Published as a conference paper at ICLR 2025
unbiased covariance matching objective that enables a neural network to match the diagonal of the
optimal state-dependent diagonal covariance. Unlike previous methods (Bao et al., 2022a; Nichol &
Dhariwal, 2021), which learn the diagonal covariance directly from the data, our approach estimates
the diagonal covariance from the learned score function. We show that this method significantly
reduces covariance estimation errors compared to existing techniques. Moreover, we demonstrate
that our approach can be applied to both Markovian (DDPM) and non-Markovian (DDIM) diffusion
models, as well as latent diffusion models. This results in improvements in generation quality, recall,
and likelihood evaluation, while also reducing the number of function evaluations (NFEs).
2 B ACKGROUND OF PROBABILISTIC DIFFUSION MODELS
We first introduce two classes of diffusion models that will be explored further in our paper.
2.1 M ARKOVIAN DIFFUSION MODEL : DDPM
Let q(x0) be the true data distribution, denoising diffusion probabilistic models (DDPM) (Ho et al.,
2020) constructs the following Markovian process
q(x0:T ) = q(x0)
TY
t=1
q(xt|xt−1), (1)
where q(xt|xt−1) = N (√1 − βtxt−1, βtI) and β1:T is the pre-defined noise schedule. We can also
derive the following skip-step noising distribution in a closed form
q(xt|x0) = N (√¯αtx0, (1 − ¯αt)I), (2)
where ¯αt ≡ Qt
s=1(1 − βs). When T is sufficiently large, we have the marginal distribution
q(xT ) → N (0, I). The generation process utilizes an initial distribution q(xT ) and the denoising
distribution q(xt−1|xt). We assume for a large T and variance preversing schedule discussed in
Equation 1, we can approximate q(xT ) ≈ p(xT ) = N (0, I). The true q(xt−1|xt) is intractable and
is approximated as a variational distribution pθ(xt−1|xt) within a Gaussian family, which defines the
reverse denoising process as
pθ(xt−1|xt) = N (xt−1|µt−1(xt; θ), Σt−1(xt; θ)). (3)
With Tweedie’s Lemma (Efron, 2011; Robbins, 1992), we can have the following score representation
µt−1(xt; θ) = (xt + βt∇xt log pθ(xt))/
p
1 − βt, (4)
where the approximated score function ∇xt log pθ(xt) ≈ ∇ xt log q(xt) can be learned by the
denoising score matching (DSM) (Vincent, 2011; Song & Ermon, 2019).
For the covariance Σt−1(xt; θ), two heuristic choices were proposed in the original DDPM paper:
1. Σt−1(xt; θ) = βt, which is equal to the variance of q(xt|xt−1) and 2. Σt−1(xt; θ) = ˜βt where
˜βt = (1 − ¯αt−1)/(1 − ¯αt)βt is the variance of q(xt−1|xt, x0). Although heuristic, Ho et al. (2020)
show that when T is large, both options yield similar generation quality.
2.2 N ON-M ARKOVIAN DIFFUSION MODEL : DDIM
In additional to the Markovian diffusion process, denoising diffusion implicit models (DDIM) (Song
et al., 2020) only defines the condition q(xt|x0) = N (√¯αtx0, (1 − ¯αt)I) and let
q(xt−1|xt) ≈
Z
q(xt−1|xt, x0)pθ(x0|xt)dx0, (5)
where q(xt−1|xt, x0) = N (µt−1, σ2
t−1) with
µt−1 = √¯αt−1x0 +
q
1−¯αt−1−σ2
t−1(xt−√¯αtx0)/
√
1 − ¯αt. (6)
When σt−1 =
p
(1 − ¯αt−1)/(1 − ¯αt)βt, the diffusion process becomes Markovian and equivalent
to DDPM. Specifically, DDIM chooses σ → 0, which implicitly defines a non-Markovian diffusion
2


=== PAGE 3 ===
Published as a conference paper at ICLR 2025
process. In the original paper (Song et al., 2020), the q(x0|xt) is heuristically chosen as a delta
function pθ(x0|xt) = δ(x0 − µ0(xt; θ)) where
µ0(xt; θ) = (xt + (1 − ¯αt)∇xt log pθ(xt))/√¯αt. (7)
In both DDPM and DDIM, the covariance ofpθ(xt−1|xt) or pθ(x0|xt) are chosen based on heuristics.
Nichol & Dhariwal (2021) have shown that the choice of covariance makes a big impact when T is
small. Therefore, for the purpose of accelerating the diffusion sampling, our paper will focus on how
to improve the covariance estimation quality in these cases. We will first introduce our method in the
next section and then compare it with other methods in Section 4.
3 D IFFUSION MODELS WITH OPTIMAL COVARIANCE MATCHING
Recently, Zhang et al. (2024b) introduce the optimal covariance form of the denoising distribution
q(x|˜x) ∝ q(˜x|x)q(x) for the Gaussian convolution q(˜x|x) = N (x, σ2I), which can be seen as a
high-dimensional extension of the second-order Tweedie’s Lemma (Efron, 2011; Robbins, 1992). We
further extend the formula to scaled Gaussian convolutions in the following theorem.
Theorem 1 (Generalized Analytical Covariance Identity) . Given a joint distribution q(˜x, x) =
q(˜x|x)q(x) with q(˜x|x) = N (αx, σ2I), then the covariance of the true posterior q(x|˜x) ∝
q(x)q(˜x|x), which is defined as Σ(˜x) = Eq(x|˜x)[x2] − Eq(x|˜x)[x]2, has a closed form:
Σ(˜x) =
 
σ4∇2
˜x log q(˜x) + σ2I

/α2. (8)
See Appendix A.1 for a proof. This covariance form can also be shown as the optimal covariance
form under the KL divergences (Bao et al., 2022b; Zhang et al., 2024b). We can see that the exact
covariance in this case only depends on the score function, which indicates the exact covariance can
be derived from the score function. In general, the score function already contains all the information
of the denoising distribution q(x|˜x), see Zhang et al. (2024b) for further discussion. We here only
consider the covariance for simplicity.
Table 1: Comparison of different covariance choices on CI-
FAR10 (CS). We use Rademacher estimator (M = 100) for
the diagonal covariance approximation in OCM-DDPM.
FiD↓ NLL↓
# timesteps 5 10 15 20 5 10 15 20
DDPM,˜β 58.28 34.76 24.02 19.00 203.29 74.95 44.94 32.20
DDPM,β 254.07 205.31 149.67 109.81 7.33 6.51 6.06 5.77
OCM-DDPM38.88 21.60 13.35 9.75 6.82 4.98 4.62 4.43
In the case of the diffusion model, we
use the learned score function as a
plug-in approximation in Equation (8).
Although the optimal covariance can
be directly calculated from the learned
score function, but it requires calculat-
ing the Hessian matrix, which is the
Jacobian of the score function. This
requires O(D2) storage and D num-
ber of network evaluation (Martens
et al., 2012) for each denoising step at the time t. Zhang et al. (2024b) propose to use the following
consistent diagonal approximation (Bekas et al., 2007) to remove the O(D2) storage requirement
diag(H(˜x)) ≈ 1/M
XM
m=1
vm ⊙ H(˜x)vm, (9)
where H(˜x) ≡ ∇2
˜x log q(˜x) and vm∼p(v) is a Rademacher random variable (Hutchinson, 1990) with
entries ±1 and ⊙ denotes the element-wise product. In Table 1, we compare the generation quality of
DDPM using different covariance choices. The results demonstrate that generation quality improves
significantly when using the Rademacher estimator to estimate the optimal covariance, compared
to the heuristic choices of β and ˜β. However, as shown in Figure 5 in the Appendix, achieving a
desirable approximation on the CIFAR10 dataset necessitates M ≥ 100 Rademacher samples. Each
calculation of vm ⊙ H(˜x)vm requires a forward pass and a backward propagation, leading to roughly
2M network evaluations in total. This significantly slows down the generation speed, making it
impractical for diffusion models. Inspired by Nichol & Dhariwal (2021); Bao et al. (2022a) and also
the amortization technique used in variational inference (Kingma & Welling, 2013; Dayan et al.,
1995), we propose to use a network to match the diagonal Hessian, which only requires one network
pass to predict the diagonal Hessian in the generation time and can be done in parallel with the
score/mean predictions with no extra time cost. In the next section, we introduce a novel unbiased
objective to learn the diagonal Hessian from the learned score, which improves the covariance
estimation accuracy and leads to better generation quality and higher likelihood estimations.
3


=== PAGE 4 ===
Published as a conference paper at ICLR 2025
Algorithm 1 Sampling procedure from t →
t′ in OCM-DDPM
1: Compute µt′(xt) with Equation (15);
2: Compute Σt′(xt) with Equation (16);
3: Sample ϵ ∼ N(0, I);
4: Let xt′ ← µt′(xt) + Σt′(xt)1/2ϵ.
Algorithm 2 Sampling procedure from t →
t′ in OCM-DDIM
1: Compute µ0(xt) with Equation (18);
2: Compute Σ0(xt) with Equation (19);
3: Sample x0 ∼ N(µ0(xt),Σ0(xt));
4: Let xt′ ← √¯αt′ x0+ √1 − ¯αt′ xt−√¯αtx0√1−¯αt
.
3.1 U NBIASED OPTIMAL COVARIANCE MATCHING
To train a network hϕ(˜x) to match the Hessian diagonal diag(H(˜x)), a straightforward solution is to
directly regress Equation 9 for all the noisy data
min
ϕ
Eq(˜x)||hϕ(˜x)− 1
M
MX
m=1
vm ⊙H(˜x)vm||2
2, (10)
where vm ∼ p(v). Although this objective is consistent when M → ∞, it will introduce additional
bias when M is small. To avoid the bias, we propose the following unbiased optimal covariance
matching (OCM) objective
Locm(ϕ) = Eq(˜x)p(v)||hϕ(˜x) − v ⊙ H(˜x)v||2
2, (11)
which does not include an expectation within the non-linear L2 norm. The following theorem shows
the validity of the proposed OCM objective.
Theorem 2 (Validity of the OCM objective). The objective in Equation (11) upper bounded the base
objective (i.e., Equation (10) with M → ∞). Moreover, it attains optimal whenhϕ(˜x) = diag(H(˜x))
for all ˜x ∼ q(˜x).
See the Appendix A.2 for a proof. The integration over v in Equation (11) can be unbiasedly
approximated by the Monte-Carlo integration given M Rademacher samples vm ∼ p(v). In practice,
we found M = 1 works well (see Table 11 in the appendix for the ablation study on varying M
values), which also shows the training efficiency of the proposed OCM objective. The learned hϕ can
form the covariance approximation
Σ(˜x; ϕ) = (σ4hϕ(˜x) + σ2I)/α2. (12)
We then discuss how to apply the learned covariance to diffusion models.
3.2 D IFFUSION MODELS APPLICATIONS
Given access to a learned score function, ∇xt log pθ(xt), from any pre-trained diffusion model, we
denote the Jacobian of the score as Ht(xt). Assuming M = 1 in the OCM training objective, the
covariance learning objective for diffusion models can be expressed as follows:
min
ϕ
1
T
TX
t=1
Eq(xt,x0)p(v)∥hϕ(xt) − v⊙Ht(xt)v∥2
2, (13)
where v ∼ p(v) and hϕ(xt) is a network that conditioned on the state xt and time t. After training
this objective, the learned hϕ(xt) can be used to form the diagonal Hessian approximation hϕ(˜x) ≈
diag(H(˜x)) which further forms our approximation of covariance. We then derive its use cases in
skip-step DDPM and DDIM for Diffusion acceleration.
Skip-Step DDPM: For the general skip-step DDPM with denoising distributionq(xt′|xt) with t′ < t.
When t′ = t − 1, this becomes the classic one-step DDPM. We further denote ¯αt′:t =Qt
s=t′ αs, and
thus ¯α0:t = ¯αt, ¯αt′:t = ¯αt/¯αt′. We can write the forward process as
q(xt|xt′) = N (xt|√¯αt′:txt′, (1 − ¯αt′:t)I). (14)
The corresponding Gaussian denoising distribution pθ,ϕ(xt′|xt) = N (µt′(xt; θ), Σt′(xt; ϕ)) has the
following mean and covariance functions:
µt′(xt; θ)=( xt + (1 − ¯αt′:t)∇xt log pθ(xt))/√¯αt′:t, (15)
Σt′(xt; ϕ)=((1 − ¯αt′:t)2hϕ(xt)+(1 − ¯αt′:t)I)/¯αt′:t. (16)
4


=== PAGE 5 ===
Published as a conference paper at ICLR 2025
Table 2: Overview of different covariance estimation methods. Methods are ranked by increasing
modeling capability from top to bottom. We also include the intuition of the methods and how many
additional network passes are required for estimating the covariance.
Modeling Capability
Covariance Type +#Passes Intuition
xt-independent Isotropicβ (Ho et al., 2020) 0 Cov. of q(xt|xt−1)
xt-independent Isotropic˜β (Ho et al., 2020) 0 Cov. of q(xt|xt−1, x0)
xt-independent Isotropic Estimation (Bao et al., 2022b) 0 Estimate from data
xt-dependent Diagonal VLB (Nichol & Dhariwal, 2021) 1 Learn from data
xt-dependent Diagonal NS (Bao et al., 2022a) 1 Learn from data
xt-dependent Diagonal OCM (Ours) 1 Learn from score
xt-dependent Diagonal Estimation (Zhang et al., 2024b) 2M Estimate from score
xt-dependent Diagonal Analytic (Zhang et al., 2024b) D Calculate from score
xt-dependent Full Analytic (Zhang et al., 2024b) D Calculate from score
The skip-step denoising sample xt′ ∼ pθ,ϕ(xt′|xt) can be obtained by xt′ = µt′(xt; θ) +
ϵΣ1/2
t′ (xt; ϕ).
Skip-Step DDIM: Similarly, we give the skip-step formulation of DDIM, where we set the σt−1 =
0 in Equation (6) which is also used in the original paper Song et al. (2020). We can use the
approximated covariance of pθ,ϕ(x0|xt) to replace the delta function used in the vanilla DDIM,
which gives the skip-steps DDIM sample
xt′ =
√
¯αt′x0 +
√
1 − ¯αt′/
√
1 − ¯αt · (xt − √¯αtx0), (17)
where x0 ∼ pθ,ϕ(x0|xt) = N (µ0(xt; θ), Σ0(xt; ϕ)) and
µ0(xt; θ) = (xt + (1 − ¯αt)∇xt log pθ(xt))/√¯αt, (18)
Σ0(xt; ϕ) = ((1 − ¯αt)2hϕ(xt, t) + (1 − ¯αt)I)/¯αt. (19)
The sample x0 ∼ pθ,ϕ(x0|xt) can be obtained by x0 = µ0(xt; θ) +ϵΣ1/2
0 (xt; ϕ). In the next section,
we will compare the proposed method to other covariance estimation methods in practical examples.
3.3 D ETAILS OF TRAINING AND INFERENCE
Our model comprises two components: a score prediction network sθ and a diagonal Hessian
prediction network hϕ. In line with Bao et al. (2022a), we parameterize the score prediction network
using a pretrained diffusion model, and the Hessian prediction network is parameterized by sharing
parameters as follows:
sθ(xt) = NN1(BaseNet(xt, t; θ1); θ2), h ϕ(xt) = NN2(BaseNet(xt, t; θ1); ϕ) (20)
where BaseNet represents the commonly used architecture in diffusion models, such as UNet and DiT
(Ho et al., 2020; Peebles & Xie, 2023). This parameterization approach only requires an additional
small neural network, NN2, resulting in negligible extra computational and memory costs compared
to the original diffusion models (see Appendix B.1 for more details). In our experiment, we fix the
parameter θ = {θ1, θ2} and train the Hessian prediction network exclusively with the proposed OCM
objective. After training, samples can be generated using Algorithms 1 and 2.
4 R ELATED COVARIANCE ESTIMATION METHODS
We then discuss different choices ofΣt′(xt) used in the diffusion model literature, see also Table 2
for an overview. For brevity, we mainly focus on DDPM here, in which the mean µt′(xt) can be
computed as in Equation (15).
1. xt-independent isotropic covariance: β/ ˜β-DDPM (Ho et al., 2020). The β-DDPM uses the
variance of p(xt′|xt), which is Σt′(xt) = (1 − ¯αt/¯αt′)I, when t′ = t − 1, we have Σt−1(xt) = βt.
The ˜β-DDPM uses the covariance of p(xt′|x0, xt), which is (1−¯αt′)
(1−¯αt) (1 − ¯αt′:t).
5


=== PAGE 6 ===
Published as a conference paper at ICLR 2025
2. xt-independent isotropic covariance: A-DDPM (Bao et al., 2022b). A-DDPM assumes a
state-independent isotropic covariance Σt′(xt) = σ2
t′I with the following analytic form of
σ2
t′ = 1− ¯αt′:t
¯αt′:t
− (1− ¯αt′:t)2
d¯αt′:t
Eq(xt)
h
∥∇xtlog pθ(xt)∥2
2
i
,
where the integration of q(xt) requires a Monte Carlo estimation before conducting generation. This
variance choice is optimal under the KL divergence within a constrained isotropic state-independent
posterior family.
3. xt-dependent diagonal covariance: I-DDPM (Nichol & Dhariwal, 2021). In I-DDPM, the
diagonal covariance matrix is modelled as the interpolation between βt and ˜βt
Σt−1(xt; ψ) = exp(vψ(xt) logβt + (1 − vψ(xt)) log ˜βt), (21)
where vψ is parametrized via a neural network. The covariance is learned with the variational lower
bound (VLB). In the optimal training, the covariance learned by VLB will recover the true covariance
in (8). Notably, Nichol & Dhariwal (2021) heuristically obtain the covariance of skip sampling
Σt′(xt; ψ) by rescaling βt and ˜βt accordingly: βt → 1 − ¯αt′:t, ˜βt → (1−¯αt′)
(1−¯αt) (1 − ¯αt′:t). However,
when t′ = 0, Σ0(xt; ψ) is ill-defined; thus, iDDPM is inapplicable within the DDIM framework.
4. xt-dependent diagonal covariance: SN-DDPM (Bao et al., 2022a). SN-DDPM learns the
covariance by training a neural network gψ to estimate the second moment of the noise ϵt =
(xt − √¯αtx0)/√1 − ¯αt:
min
ψ
Et,q(x0,xt)∥ϵ2
t − gψ(xt)∥2
2. (22)
After training, the covariance Σt′(xt; ψ) can be estimated via
Σt′(xt; ψ)= (1− ¯αt′)
(1 − ¯αt)
¯βt′:tI+ β2
t
αt
gψ(xt)
1− ¯αt
−∇xt logpθ(xt)2

. (23)
where ¯βt′:t = 1 − ¯αt′:t. In optimal, Σt′(xt; ψ) in Equation (23) will recover the true covariance
in Equation (8). We demonstrate the equivalence between OCM-DDPM and SN-DDPM in Ap-
pendix A.3. However, due to the appearance of the quadratic term in Equation (23), SN-DDPM tends
to amplify the estimation error as t → 0, leading to suboptimal solutions. To mitigate this issue, Bao
et al. (2022a) also propose NPR-DDPM, which models the noise prediction residual instead. We
recommend referring to their paper for detailed explanations.
Notably, almost all these methods can be applied within the DDIM framework by setting t′ = 0.
Specifically, p(xt′|xt) can be sampled using Equation (17) with x0 ∼ N (µ0(xt), Σ0(xt)), where
µ0(xt) is the same as in (18), but Σ0(xt) differs for various methods as discussed previously.
5 E XPERIMENTAL RESULTS
To support our theoretical discussion, we first evaluate the performance of optimal covariance
matching by training diffusion probabilistic models on 2D toy examples. We then demonstrate
its effectiveness in enhancing image modelling in both pixel and latent spaces, focusing on the
comparison between optimal covariance matching and other covariance estimation methods, and
showing that the proposed approach has the potential to scale to large image generation tasks.
5.1 T OY DEMONSTRATION
We first demonstrate the effectiveness of our method by considering the data distribution as a two-
dimensional mixture of forty Gaussians (MoG) with means uniformly distributed over [−40, 40] ⊗
[−40, 40] and a standard deviation of σ =
√
40, where ⊗ denotes Cartesian product (see Figure 1a
for visualization). In this case, both the true score ∇xt log p(xt) and the optimal covariance Σt′(xt)
are available, allowing us to compare the covariance estimation error. We then learn the covariance
using the true scores by different methods to conduct DDPM and DDIM sampling for this MoG
6


=== PAGE 7 ===
Published as a conference paper at ICLR 2025
Figure 2: The results of FID v.s. NLL for different methods with varying numbers of sampling steps
on CIFAR10 (CS). Our method consistently achieves the best trade-off between FID and NLL.
problem. For evaluation, we employ the Maximum Mean Discrepancy (MMD) (Gretton et al., 2012),
which utilizes five kernels with bandwidths{2−2, 2−1, 20, 21, 22}. In Figures 4c and 4d, we show the
MMD comparison among different methods. Specifically, we choose to conduct different diffusion
steps with the skip-step scheme as discussed in Section 3.2.
(a) Training Data and Density
 (b) DDPM MMD v.s. Steps
 (c) DDIM MMD v.s. Steps
Figure 1: Comparisons of different covariance estimation methods.
Figure (a) demonstrates the training data and the ground truth density.
Figures (b) and (c) present the MMD evaluation against the total
sampling steps in the DDPM (b) and DDIM (c) settings.
The results, shown in Fig-
ures 1b and 1c, demonstrate
that the proposed method
outperforms other covariance
learning approaches. Addi-
tionally, we include two meth-
ods utilizing the true diago-
nal and full covariance, serv-
ing as benchmarks for the best
achievable performance. No-
tably, in this case, using the
full covariance yields better
performance compared to the
diagonal covariance. This ob-
servation highlights the importance of accurate covariance approximation, suggesting that improved
methods for learning the off-diagonal terms could lead to even better results. For further analysis, we
present an additional toy experiment in Appendix C.1 to demonstrate the efficacy of our approach.
5.2 I MAGE MODELLING WITH DIFFUSION MODELS
Following the experimental setting in Bao et al. (2022a), we then evaluate our method across varying
pre-trained score networks provided by previous works (Ho et al., 2020; Nichol & Dhariwal, 2021;
Song et al., 2020; Bao et al., 2022b; Peebles & Xie, 2023). In this experiment, we mainly focus on
four datasets: CIFAR10 (Krizhevsky et al., 2009) with the linear schedule (LS) of βt (Ho et al., 2020)
and the cosine schedule (CS) of βt (Nichol & Dhariwal, 2021); CelebA (Liu et al., 2015); LSUN
Bedroom (Yu et al., 2015). The details of the experimental setting can be found in Appendix B.
IMPROVING LIKELIHOOD RESULTS
We first evaluate the negative log-likelihood (NLL) of the diffusion models by calculating the negative
evidence lower bound (ELBO): − log q(x0) ≤ Eq log p(x0:T )
q(x1:T |x0) ≡ −Lelbo(x0) with the same mean
yet varying covariances. As per Bao et al. (2022b), we report results only for the DDPM forward
process, as −Lelbo = ∞ under the DDIM forward process. As shown in Table 3, our OCM-DDPM
demonstrates the best or second-best performance in most scenarios, highlighting the effectiveness
of our covariance estimation approach. We also note that SN-DDPM performs poorly on likelihood
results in small-scale datasets like CIFAR10 and CELEBA, likely due to the amplified error of the
quadratic term in Equation (22). Although NPR-DDPM achieves slightly better performance in
certain scenarios, it falls short in terms of sample quality, as measured by FID, shown in Table 5.
In Appendix C.2, we also compare our method to Improved DDPM (Nichol & Dhariwal, 2021) in
terms of likelihood on ImageNet 64x64. The results show that our method achieves better likelihood
estimation with a small number of timesteps, while achieving comparable results with full timesteps.
7


=== PAGE 8 ===
Published as a conference paper at ICLR 2025
Table 3: The NLL (bits/dim) ↓ across various datasets using different sampling steps.
CIFAR10 (LS) CIFAR10 (CS)
# TIMESTEPS K 10 25 50 100 200 1000 10 25 50 100 200 1000
DDPM, ˜β 74.95 24.98 12.01 7.08 5.03 3.73 75.96 24.94 11.96 7.04 4.95 3.60
DDPM, β 6.99 6.11 5.44 4.86 4.39 3.75 6.51 5.55 4.92 4.41 4.03 3.54
A-DDPM 5.47 4.79 4.38 4.07 3.84 3.59 5.08 4.45 4.09 3.83 3.64 3.42
NPR-DDPM 5.40 4.64 4.25 3.98 3.79 3.57 5.03 4.33 3.99 3.76 3.59 3.41
SN-DDPM 30.79 11.83 7.13 5.24 4.39 3.74 90.85 19.81 9.72 6.72 5.58 4.73
OCM-DDPM 5.32 4.63 4.25 3.97 3.78 3.57 4.99 4.34 3.99 3.76 3.59 3.41
CELEB A 64 X64 I MAGE NET 64X64
# TIMESTEPS K 10 25 50 100 200 1000 25 50 100 200 400 4000
DDPM, ˜β 33.42 13.09 7.14 4.60 3.45 2.71 105.87 46.25 22.02 12.10 7.59 3.89
DDPM, β 6.67 5.72 4.98 4.31 3.74 2.93 5.81 5.20 4.70 4.31 4.04 3.65
A-DDPM 4.54 3.89 3.48 3.16 2.92 2.66 4.78 4.42 4.15 3.95 3.81 3.61
NPR-DDPM 4.46 3.78 3.40 3.11 2.89 2.65 4.66 4.22 3.96 3.80 3.71 3.60
SN-DDPM 18.09 8.05 5.29 4.05 3.40 2.84 4.56 4.18 3.95 3.80 3.71 3.63
OCM-DDPM 4.69 3.86 3.43 3.13 2.90 2.66 4.45 4.15 3.93 3.79 3.70 3.59
IMPROVING SAMPLE QUALITY
Next, we compare sample quality quantitatively using the FID score, with results reported for both
the DDPM and DDIM forward processes. As shown in Table 5, our OCM-DPM consistently achieves
either the best or second-best results across most settings. The qualitative results of the samples
generated by our method can be found in Appendix C.6.
Notably, while SN-DDPM often demonstrates slightly better FID performance in certain cases, it
struggles with likelihood estimation, as shown in Table 3. Conversely, as discussed in the previous
section, NPR-DDPM sometimes achieves superior likelihood results in Table 3, but its image
generation quality is significantly worse than OCM-DDPM. Therefore, the proposed OCM-DDPM
provides a more balanced model, offering a better trade-off between generation quality and likelihood
estimation. To better highlight this benefit, we include a visualization in Figure 2.
5.3 I MAGE MODELLING WITH LATENT DIFFUSION MODELS
1.5 1.75 2.0 3.0 4.0
CFG
5
10
15
20
25
30
35
40
45FID
DDPM, 
I-DDPM
DDIM
OCM-DDPM
OCM-DDIM
1.5 1.75 2.0 3.0 4.0
CFG
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55Recall
Figure 3: Results of DiT training on ImageNet 256x256.
We generate samples using 10 timesteps with varying CFG
coefficients (see Table 12 for exact numerical values).
In this section, we apply our meth-
ods to latent diffusion models (Vahdat
et al., 2021; Rombach et al., 2022) and
with ImageNet 256×256 to demon-
strate the scalability of our method.
Specifically, we compare our methods
to other approaches within the DiT
architecture (Peebles & Xie, 2023),
evaluating sample quality using the
FID score and diversity using the Re-
call score (Sajjadi et al., 2018). We
focus on conditional generation with
classifier-free guidance (CFG), which is consistent to Peebles & Xie (2023).
It is important to note that DiT was initially trained using the Improved DDPM (I-DDPM) algorithm
(Nichol & Dhariwal, 2021). Therefore, the results of I-DDPM reflect the original performance of
the pre-trained DiT model1 provided by Peebles & Xie (2023). For DDPM and DDIM sampling,
we discard the learned covariance in DiT and use only the learned noise prediction neural network
for the posterior mean. In contrast, our OCM-DPM retains the same mean function while learning
the covariance using the optimal covariance matching objective. We then generate samples with 10
sampling steps and report FID and Recall scores across different CFG coefficients on the ImageNet
256x256 dataset. The results are displayed in Figure 3, with DDPM- β excluded due to its poor
performance. It shows that our OCM-DPM method achieves the best FID performance with CFG =
2.0. Although DDPM- ˜β and DDIM perform better in terms of FID at CFG = 1.5, their Recall scores
1We use the DiT XL/2 checkpoint released in https://github.com/facebookresearch/DiT.
8


=== PAGE 9 ===
Published as a conference paper at ICLR 2025
Table 4: The least number of timesteps ↓ required to achieve an FID around 6 (along with the
corresponding FID). To account for the additional time cost incurred by the covariance prediction
network, we multiply the results by the ratio of the time cost per single timestep, reflecting the extra
computational overhead (see Appendix B.1 for details).
METHOD CIFAR10 C ELEB A 64 X64 LSUN B EDROOM IMAGE NET 256 X256
DDPM 90 (6.12) > 200 130 (6.06) 21 (5.89)
DDIM 30 (5.85) > 100 B EST FID > 6 11 (5.58)
IMPROVED DDPM 45 (5.96) M ISSING MODEL 90 (6.02) 22 (6.08)
ANALYTIC -DPM 25 (5.81) 55 (5.98) 100 (6.05) M ISSING MODEL
NPR-DPM 1.002 ×23 (5.76) 1.013 ×50 (6.04) 1.021 ×90 (6.01) M ISSING MODEL
SN-DPM 1.005 ×17 (5.81) 1.019 ×22 (5.96) 1.114 ×92 (6.02) M ISSING MODEL
OCM-DPM ( OURS ) 1.003 ×16 (5.83) 1.015 ×21 (5.94) 1.112 ×90 (6.04) 1.007 ×10 (5.33)
are lower than those of the proposed OCM methods. This discrepancy arises partly because DDPM
and DDIM model the inverse process with deterministic variance, whereas OCM methods explicitly
learn the variance from data, allowing for more accurate density estimation and generating more
diverse results. As Peebles & Xie (2023) report that DiT achieves the best FID performance with
CFG = 1.5, we further evaluate performance with different sampling steps under CFG = 1.5, as
shown in Table 13. The results indicate that our OCM-DPM methods offer a great improvement in
sample quality and diversity.
To provide an overview of the superiority of our method, we compare the minimum sampling step to
achieve the FID score (Heusel et al., 2017) close to 6 across different approaches (see Appendix B.1
for details). As shown in Table 4, our method requires the fewest denoising steps in most settings.
Moreover, we emphasize that compared to recent advanced methods like SN-DPM and NPR-DPM,
our approach is the only one that consistently delivers both competitive likelihood and FID.
6 R ELATED WORK AND FUTURE DIRECTIONS
In this paper, we show that improving diagonal covariance estimation can significantly enhance
the performance of diffusion models. This raises a natural question: can more flexible covariance
structures further improve these models? In Figure 1, we demonstrate that a full covariance structure
achieves better generation quality with fewer NFEs in a toy 2D problem. However, for high-
dimensional problems, the quadratic growth in parameter scale makes a full covariance approach
computationally impractical. To address this, low-rank or block-diagonal covariance approximations
offer promising alternatives. Developing effective training objectives for flexible covariance structures
remains a compelling direction for future research.
In addition to the covariance estimation methods discussed in Section 4, there are other approaches to
accelerate sampling in diffusion models. One approach involves using faster numerical solvers for
differential equations with continuous timesteps (Jolicoeur-Martineau et al., 2021; Liu et al., 2022;
Lu et al., 2022). Another strategy, inspired by Schrödinger bridge (Wang et al., 2021; De Bortoli
et al., 2021), is to introduce a nonlinear, trainable forward diffusion process. Additionally, replacing
Gaussian modelling of pθ(x0|xt) with more expressive alternatives, such as GANs (Xiao et al., 2021;
Wang et al., 2022), distributional models (Bortoli et al., 2025), latent variable models (Yu et al., 2024),
or energy-based models (Xu et al., 2024), can also accelerate the sampling of diffusion models.
Recently, distillation techniques have gained popularity, achieving state-of-the-art in one-step genera-
tion (Zhou et al., 2024). There are two prominent types of distillation methods. The first is trajectory
distillation (Salimans & Ho, 2022; Berthelot et al., 2023; Song et al., 2023; Heek et al., 2024; Kim
et al., 2023; Li & He, 2024), which focuses on accelerating the process of solving differential equa-
tions. The second type involves distillation techniques that utilize a one-step implicit latent variable
model as the student model (Luo et al., 2024; Salimans et al., 2024; Xie et al., 2024; Zhou et al.,
2024; Zhang et al., 2025) and distills the diffusion process into the student model by minimizing the
spread divergence family (Zhang et al., 2020; 2019) through score estimation (Poole et al., 2022;
Wang et al., 2024).
Although these distillation methods typically offer faster generation speeds (fewer NFEs) compared
to covariance estimation methods, they often lack tractable density or likelihood estimation. This
presents a challenge for generative modeling applications where likelihood or density estimation is
9


=== PAGE 10 ===
Published as a conference paper at ICLR 2025
Table 5: FID score ↓ across various datasets using different sampling steps.
CIFAR10 (LS) CIFAR10 (CS)
# TIMESTEPS K 10 25 50 100 200 1000 10 25 50 100 200 1000
DDPM, ˜β 44.45 21.83 15.21 10.94 8.23 5.11 34.76 16.18 11.11 8.38 6.66 4.92
DDPM, β 233.41 125.05 66.28 31.36 12.96 3.04 205.31 84.71 37.35 14.81 5.74 3.34
A-DDPM 34.26 11.60 7.25 5.40 4.01 4.03 22.94 8.50 5.50 4.45 4.04 4.31
NPR-DDPM 32.35 10.55 6.18 4.52 3.57 4.10 19.94 7.99 5.31 4.52 4.10 4.27
SN-DDPM 24.06 6.91 4.63 3.67 3.31 3.65 16.33 6.05 4.17 3.83 3.72 4.07
OCM-DDPM 24.94 9.19 5.95 4.36 3.48 3.98 14.32 5.54 4.10 3.84 3.75 4.18
DDIM 21.31 10.70 7.74 6.08 5.07 4.13 34.34 16.68 10.48 7.94 6.69 4.89
A-DDIM 14.00 5.81 4.04 3.55 3.39 3.74 26.43 9.96 6.02 4.88 4.92 4.66
NPR-DDIM 13.34 5.38 3.95 3.53 3.42 3.72 22.81 9.47 6.04 5.02 5.06 4.62
SN-DDIM 12.19 4.28 3.39 3.23 3.22 3.65 17.90 7.36 5.16 4.63 4.63 4.51
OCM-DDIM 10.66 4.35 3.48 3.27 3.29 3.74 16.70 6.71 4.72 4.30 4.54 4.53
CELEB A 64 X64 I MAGE NET 64X64
# TIMESTEPS K 10 25 50 100 200 1000 25 50 100 200 400 4000
DDPM, ˜β 36.69 24.46 18.96 14.31 10.48 5.95 29.21 21.71 19.12 17.81 17.48 16.55
DDPM, β 294.79 115.69 53.39 25.65 9.72 3.16 170.28 83.86 45.04 28.39 21.38 16.38
A-DDPM 28.99 16.01 11.23 8.08 6.51 5.21 32.56 22.45 18.80 17.16 16.40 16.34
NPR-DDPM 28.37 15.74 10.89 8.23 7.03 5.33 28.27 20.89 18.06 16.96 16.32 16.38
SN-DDPM 20.60 12.00 7.88 5.89 5.02 4.42 27.58 20.74 18.04 16.61 16.37 16.22
OCM-DDPM 21.55 12.71 9.24 6.97 5.92 5.04 28.02 20.81 17.98 16.74 16.32 16.31
DDIM 20.54 13.45 9.33 6.60 4.96 3.40 26.06 20.10 18.09 17.84 17.74 19.00
A-DDIM 15.62 9.22 6.13 4.29 3.46 3.13 25.98 19.23 17.73 17.49 17.44 18.98
NPR-DDIM 14.98 8.93 6.04 4.27 3.59 3.15 28.84 19.62 17.63 17.42 17.30 18.91
SN-DDIM 10.20 5.48 3.83 3.04 2.85 2.90 28.07 19.38 17.53 17.23 17.23 18.89
OCM-DDIM 10.28 5.72 4.42 3.54 3.17 3.03 28.28 19.62 17.71 17.42 17.26 19.02
crucial. For example, in AI4Science problems involving energy-based data, accurate model density
estimation is essential for enabling importance sampling to correct sample bias, as discussed in (Zhang
et al., 2024a; Vargas et al., 2023; Chen et al., 2024; Akhound-Sadegh et al., 2024). Another example
is diffusion model-based data compression, where better likelihood corresponds to better compression
rates (Townsend et al., 2019; Zhang et al., 2021; Ho et al., 2020; Kingma et al., 2021). In these cases,
the proposed OCM-DDPM can be used to provide better likelihood estimates, resulting in improved
task performance. Additionally, a recent paper (Salimans et al., 2024) shows how moment-matching
improves distillation, achieving state-of-the-art diffusion quality with first-order matching. Our
method could extend this to higher-order moment matching, potentially accelerating distillation
training. We leave it as a promising direction for future work.
Beyond image modelling, our method can be straightforwardly applied to accelerate large-scale video
diffusion models (Blattmann et al., 2023; Chen et al., 2023), which are based on latent diffusion
models. A recent study (Zhao et al., 2024) shows that covariance estimation is crucial in mitigating the
image-leakage issue in image-to-video generation problems, presenting another promising application
of our method. Moreover, our method can be applied to solve inverse problems, where the optimal
covariance can enhance the accuracy of posterior sampling (Chung et al., 2022; Rozet et al., 2024).
While this work focuses on the DDPM and DDIM sampler, incorporating the improved covariance
into the general stochastic and deterministic differential solver (Song et al., 2021; Karras et al., 2022)
also represents exciting directions for future research.
7 C ONCLUSION
In this paper, we proposed a new method for learning the diagonal covariance of the denoising
distribution, which offers improved accuracy compared to other covariance learning approaches.
Our results demonstrate that enhanced covariance estimation leads to better generation quality and
diversity, all while reducing the number of required generation steps. We validated the scalability
of our method across different problem domains and scales and discussed its connection to various
acceleration techniques and highlighted several promising application areas for future exploration.
10


=== PAGE 11 ===
Published as a conference paper at ICLR 2025
8 A CKNOWLEDGMENTS
ZO is supported by the Lee Family Scholarship. MZ and DB acknowledge funding from AI Hub in
Generative Models, under grant EP/Y028805/1 and funding from the Cisco Centre of Excellence.
We want to thank Wenlin Chen for the useful discussions.
REFERENCES
Akhound-Sadegh, T., Rector-Brooks, J., Bose, A. J., Mittal, S., Lemos, P., Liu, C.-H., Sendera, M.,
Ravanbakhsh, S., Gidel, G., Bengio, Y ., et al. Iterated denoising energy matching for sampling
from boltzmann densities. arXiv preprint arXiv:2402.06121, 2024.
Bao, F., Li, C., Sun, J., Zhu, J., and Zhang, B. Estimating the optimal covariance with imperfect
mean in diffusion probabilistic models. arXiv preprint arXiv:2206.07309, 2022a.
Bao, F., Li, C., Zhu, J., and Zhang, B. Analytic-dpm: an analytic estimate of the optimal reverse
variance in diffusion probabilistic models. arXiv preprint arXiv:2201.06503, 2022b.
Bekas, C., Kokiopoulou, E., and Saad, Y . An estimator for the diagonal of a matrix. Applied
numerical mathematics, 57(11-12):1214–1229, 2007.
Berthelot, D., Autef, A., Lin, J., Yap, D. A., Zhai, S., Hu, S., Zheng, D., Talbott, W., and Gu,
E. Tract: Denoising diffusion models with transitive closure time-distillation. arXiv preprint
arXiv:2303.04248, 2023.
Blattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D., Kilian, M., Lorenz, D., Levi, Y ., English,
Z., V oleti, V ., Letts, A., et al. Stable video diffusion: Scaling latent video diffusion models to large
datasets. arXiv preprint arXiv:2311.15127, 2023.
Bortoli, V . D., Galashov, A., Guntupalli, J. S., Zhou, G., Murphy, K., Gretton, A., and Doucet, A.
Distributional diffusion models with scoring rules, 2025. URL https://arxiv.org/abs/
2502.02483.
Chen, H., Xia, M., He, Y ., Zhang, Y ., Cun, X., Yang, S., Xing, J., Liu, Y ., Chen, Q., Wang, X.,
et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint
arXiv:2310.19512, 2023.
Chen, W., Zhang, M., Paige, B., Hernández-Lobato, J. M., and Barber, D. Diffusive gibbs sampling.
arXiv preprint arXiv:2402.03008, 2024.
Chung, H., Kim, J., Mccann, M. T., Klasky, M. L., and Ye, J. C. Diffusion posterior sampling for
general noisy inverse problems. arXiv preprint arXiv:2209.14687, 2022.
Dayan, P., Hinton, G. E., Neal, R. M., and Zemel, R. S. The helmholtz machine. Neural computation,
7(5):889–904, 1995.
De Bortoli, V ., Thornton, J., Heng, J., and Doucet, A. Diffusion schrödinger bridge with applications
to score-based generative modeling. Advances in Neural Information Processing Systems , 34:
17695–17709, 2021.
Efron, B. Tweedie’s formula and selection bias. Journal of the American Statistical Association, 106
(496):1602–1614, 2011.
Gretton, A., Borgwardt, K. M., Rasch, M. J., Schölkopf, B., and Smola, A. A kernel two-sample test.
The Journal of Machine Learning Research, 13(1):723–773, 2012.
He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings
of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.
Heek, J., Hoogeboom, E., and Salimans, T. Multistep consistency models. arXiv preprint
arXiv:2403.06807, 2024.
11


=== PAGE 12 ===
Published as a conference paper at ICLR 2025
Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by a two
time-scale update rule converge to a local nash equilibrium. Advances in neural information
processing systems, 30, 2017.
Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. Advances in Neural
Information Processing Systems, 33:6840–6851, 2020.
Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., and Fleet, D. J. Video diffusion models.
Advances in Neural Information Processing Systems, 35:8633–8646, 2022.
Hoogeboom, E., Satorras, V . G., Vignac, C., and Welling, M. Equivariant diffusion for molecule
generation in 3d. In International conference on machine learning, pp. 8867–8887. PMLR, 2022.
Hutchinson, M. F. A stochastic estimator of the trace of the influence matrix for laplacian smoothing
splines. Communications in Statistics-Simulation and Computation, 19(2):433–450, 1990.
Jolicoeur-Martineau, A., Li, K., Piché-Taillefer, R., Kachman, T., and Mitliagkas, I. Gotta go fast
when generating data with score-based models. arXiv preprint arXiv:2105.14080, 2021.
Karras, T., Aittala, M., Aila, T., and Laine, S. Elucidating the design space of diffusion-based
generative models. Advances in Neural Information Processing Systems, 35:26565–26577, 2022.
Kim, D., Lai, C.-H., Liao, W.-H., Murata, N., Takida, Y ., Uesaka, T., He, Y ., Mitsufuji, Y ., and Ermon,
S. Consistency trajectory models: Learning probability flow ode trajectory of diffusion. arXiv
preprint arXiv:2310.02279, 2023.
Kingma, D., Salimans, T., Poole, B., and Ho, J. Variational diffusion models. Advances in neural
information processing systems, 34:21696–21707, 2021.
Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Kingma, D. P. and Welling, M. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114,
2013.
Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009.
Li, L. and He, J. Bidirectional consistency models. arXiv preprint arXiv:2403.18035, 2024.
Li, X., Thickstun, J., Gulrajani, I., Liang, P. S., and Hashimoto, T. B. Diffusion-lm improves
controllable text generation. Advances in Neural Information Processing Systems, 35:4328–4343,
2022.
Liu, H., Chen, Z., Yuan, Y ., Mei, X., Liu, X., Mandic, D., Wang, W., and Plumbley, M. D. Audioldm:
Text-to-audio generation with latent diffusion models. arXiv preprint arXiv:2301.12503, 2023.
Liu, L., Ren, Y ., Lin, Z., and Zhao, Z. Pseudo numerical methods for diffusion models on manifolds.
arXiv preprint arXiv:2202.09778, 2022.
Liu, Z., Luo, P., Wang, X., and Tang, X. Deep learning face attributes in the wild. In Proceedings of
the IEEE international conference on computer vision, pp. 3730–3738, 2015.
Loshchilov, I. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.
Lu, C., Zhou, Y ., Bao, F., Chen, J., Li, C., and Zhu, J. Dpm-solver: A fast ode solver for diffusion
probabilistic model sampling in around 10 steps. Advances in Neural Information Processing
Systems, 35:5775–5787, 2022.
Luo, W., Hu, T., Zhang, S., Sun, J., Li, Z., and Zhang, Z. Diff-instruct: A universal approach
for transferring knowledge from pre-trained diffusion models. Advances in Neural Information
Processing Systems, 36, 2024.
Martens, J., Sutskever, I., and Swersky, K. Estimating the hessian by back-propagating curvature.
arXiv preprint arXiv:1206.6464, 2012.
12


=== PAGE 13 ===
Published as a conference paper at ICLR 2025
Meng, C., Song, Y ., Li, W., and Ermon, S. Estimating high order gradients of the data distribution by
denoising. Advances in Neural Information Processing Systems, 34:25359–25369, 2021.
Nichol, A. Q. and Dhariwal, P. Improved denoising diffusion probabilistic models. In International
conference on machine learning, pp. 8162–8171. PMLR, 2021.
Peebles, W. and Xie, S. Scalable diffusion models with transformers. InProceedings of the IEEE/CVF
International Conference on Computer Vision, pp. 4195–4205, 2023.
Poole, B., Jain, A., Barron, J. T., and Mildenhall, B. Dreamfusion: Text-to-3d using 2d diffusion.
arXiv preprint arXiv:2209.14988, 2022.
Robbins, H. E. An empirical bayes approach to statistics. In Breakthroughs in Statistics: Foundations
and basic theory, pp. 388–394. Springer, 1992.
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis
with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition, pp. 10684–10695, 2022.
Rozet, F., Andry, G., Lanusse, F., and Louppe, G. Learning diffusion priors from observations by
expectation maximization. arXiv preprint arXiv:2405.13712, 2024.
Sajjadi, M. S., Bachem, O., Lucic, M., Bousquet, O., and Gelly, S. Assessing generative models via
precision and recall. Advances in neural information processing systems, 31, 2018.
Salimans, T. and Ho, J. Progressive distillation for fast sampling of diffusion models. arXiv preprint
arXiv:2202.00512, 2022.
Salimans, T., Mensink, T., Heek, J., and Hoogeboom, E. Multistep distillation of diffusion models
via moment matching. arXiv preprint arXiv:2406.04103, 2024.
Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning
using nonequilibrium thermodynamics. In International conference on machine learning , pp.
2256–2265. PMLR, 2015.
Song, J., Meng, C., and Ermon, S. Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502, 2020.
Song, Y . and Ermon, S. Generative modeling by estimating gradients of the data distribution.
Advances in Neural Information Processing Systems, 32, 2019.
Song, Y ., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based genera-
tive modeling through stochastic differential equations. In International Conference on Learning
Representations, 2021. URL https://openreview.net/forum?id=PxTIG12RRHS.
Song, Y ., Dhariwal, P., Chen, M., and Sutskever, I. Consistency models. arXiv preprint
arXiv:2303.01469, 2023.
Townsend, J., Bird, T., and Barber, D. Practical lossless compression with latent variables using bits
back coding. arXiv preprint arXiv:1901.04866, 2019.
Vahdat, A., Kreis, K., and Kautz, J. Score-based generative modeling in latent space. Advances in
neural information processing systems, 34:11287–11302, 2021.
Vargas, F., Grathwohl, W., and Doucet, A. Denoising diffusion samplers. arXiv preprint
arXiv:2302.13834, 2023.
Vincent, P. A connection between score matching and denoising autoencoders. Neural computation,
23(7):1661–1674, 2011.
Wang, G., Jiao, Y ., Xu, Q., Wang, Y ., and Yang, C. Deep generative learning via schrödinger bridge.
In International conference on machine learning, pp. 10794–10804. PMLR, 2021.
Wang, Z., Zheng, H., He, P., Chen, W., and Zhou, M. Diffusion-gan: Training gans with diffusion.
arXiv preprint arXiv:2206.02262, 2022.
13


=== PAGE 14 ===
Published as a conference paper at ICLR 2025
Wang, Z., Lu, C., Wang, Y ., Bao, F., Li, C., Su, H., and Zhu, J. Prolificdreamer: High-fidelity and
diverse text-to-3d generation with variational score distillation. Advances in Neural Information
Processing Systems, 36, 2024.
Xiao, Z., Kreis, K., and Vahdat, A. Tackling the generative learning trilemma with denoising diffusion
gans. arXiv preprint arXiv:2112.07804, 2021.
Xie, S., Xiao, Z., Kingma, D. P., Hou, T., Wu, Y . N., Murphy, K. P., Salimans, T., Poole, B., and Gao,
R. Em distillation for one-step diffusion models. arXiv preprint arXiv:2405.16852, 2024.
Xu, M., Geffner, T., Kreis, K., Nie, W., Xu, Y ., Leskovec, J., Ermon, S., and Vahdat, A. Energy-based
diffusion language models for text generation. arXiv preprint arXiv:2410.21357, 2024.
Yu, F., Seff, A., Zhang, Y ., Song, S., Funkhouser, T., and Xiao, J. Lsun: Construction of a large-scale
image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365,
2015.
Yu, L., Xie, T., Zhu, Y ., Yang, T., Zhang, X., and Zhang, C. Hierarchical semi-implicit variational
inference with application to diffusion model acceleration. Advances in Neural Information
Processing Systems, 36, 2024.
Zhang, F., He, J., Midgley, L. I., Antorán, J., and Hernández-Lobato, J. M. Efficient and unbiased
sampling of boltzmann distributions via consistency models. arXiv preprint arXiv:2409.07323,
2024a.
Zhang, M., Bird, T., Habib, R., Xu, T., and Barber, D. Variational f-divergence minimization. arXiv
preprint arXiv:1907.11891, 2019.
Zhang, M., Hayes, P., Bird, T., Habib, R., and Barber, D. Spread divergence. In International
Conference on Machine Learning, pp. 11106–11116. PMLR, 2020.
Zhang, M., Zhang, A., and McDonagh, S. On the out-of-distribution generalization of probabilistic
image modelling. Advances in Neural Information Processing Systems, 34:3811–3823, 2021.
Zhang, M., Hawkins-Hooker, A., Paige, B., and Barber, D. Moment matching denoising gibbs
sampling. Advances in Neural Information Processing Systems, 36, 2024b.
Zhang, M., He, J., Chen, W. C., Ou, Z., Hernández-Lobato, J. M., Schölkopf, B., and Barber, D.
Towards training one-step diffusion models without distillation. arXiv preprint arXiv:2502.08005,
2025.
Zhao, M., Zhu, H., Xiang, C., Zheng, K., Li, C., and Zhu, J. Identifying and solving conditional
image leakage in image-to-video diffusion model. arXiv preprint arXiv:2406.15735, 2024.
Zhou, M., Zheng, H., Wang, Z., Yin, M., and Huang, H. Score identity distillation: Exponentially
fast distillation of pretrained diffusion models for one-step generation. In Forty-first International
Conference on Machine Learning, 2024.
14


=== PAGE 15 ===
Published as a conference paper at ICLR 2025
Appendix for “Improved Diffusion Models with
Optimal Covariance Matching"
CONTENTS
A Abstract Proof and Derivations 15
A.1 Derivations of the Analytical Full Covariance Identity . . . . . . . . . . . . . . . . . . . . . 15
A.2 Validity of the OCM Objective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
A.3 Connection to SN-DDPM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
B Details of Experiments 18
B.1 Details of Model Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
B.2 Details of Training, Inference, and Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . 19
C Additional Experimental Results 20
C.1 Additional Toy Demonstration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
C.2 Additional Likelihood Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
C.3 Impact of the Numbers of Rademacher Samples . . . . . . . . . . . . . . . . . . . . . . . . 21
C.4 Mean and Variance of Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
C.5 More Results on Latent Diffusion Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
C.6 Generated Samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
A A BSTRACT PROOF AND DERIVATIONS
A.1 D ERIVATIONS OF THE ANALYTICAL FULL COVARIANCE IDENTITY
Theorem 1 (Generalized Analytical Covariance Identity) . Given a joint distribution q(˜x, x) =
q(˜x|x)q(x) with q(˜x|x) = N (αx, σ2I), then the covariance of the true posterior q(x|˜x) ∝
q(x)q(˜x|x), which is defined as Σ(˜x) = Eq(x|˜x)[x2] − Eq(x|˜x)[x]2, has a closed form:
Σ(˜x) =
 
σ4∇2
˜x log q(˜x) + σ2I

/α2. (8)
Proof. This proof generalizes the original analytical covariance identity proof discussed in Zhang
et al. (2024b). Using the fact that ∇˜xq(˜x|x) = − 1
σ2 (˜x − αx)q(˜x|x) and the Tweedie’s formula
∇˜x log q(˜x) = 1
σ2
 
αEq(x|˜x)[x] − ˜x

, we can expand the hessian of the log qθ(˜x):
∇2
˜x log q(˜x) = − 1
σ2
Z
∇˜x

(˜x − αx) q(˜x|x)q(x)
q(˜x)

dx
= − 1
σ2
Z q(˜x|x)q(x)
q(˜x) dx − 1
σ2
Z
(˜x − αx) ∇˜xq(˜x|x)q(˜x)q(x) − ∇˜xq(˜x)q(˜x|x)q(x)
q2(˜x) dx
=⇒ σ2∇2
˜x log q(˜x) + 1 = −
Z
(˜x − αx) ∇˜xq(˜x|x)q(x) − ∇˜x log q(˜x)q(˜x|x)q(x)
q(˜x) dx
= −
Z
(˜x − αx) − 1
σ2 (˜x − αx)q(˜x|x)q(x) + 1
σ2 (˜x − αEq(x|˜x)[x])q(˜x|x)q(x)
q(˜x) dx
=⇒ σ4∇2
˜x log q(˜x) + σ2I =
Z  
(˜x − αx)2 − (˜x − αx)(˜x − αEq(x|˜x)[x])

q(x|˜x) dx
= α2Eq(x|˜x)[x2] − α2Eq(x|˜x)[x]2 ≡ α2Σ(˜x)
Therefore, we obtain the analytical full covariance identity: Σq(˜x) =
 
σ4∇2
˜x log q(˜x) + σ2I

/α2.
15


=== PAGE 16 ===
Published as a conference paper at ICLR 2025
A.2 V ALIDITY OF THE OCM O BJECTIVE
Theorem 2 (Validity of the OCM objective). The objective in Equation (11) upper bounded the base
objective (i.e., Equation (10) with M → ∞). Moreover, it attains optimal whenhϕ(˜x) = diag(H(˜x))
for all ˜x ∼ q(˜x).
Proof. Recall that to learn the optimal covariance, we can minimize the following loss function
min
ϕ
Eq(˜x)
hϕ(˜x) − Eq(v)[v ⊙ H(˜x)v]
2
2 (24)
We call this the grounded objective because it remains unbiased and consistent, i.e., hϕ∗ =
diag(H(˜x)), when the inner expectation is approximated with infinite Monte Carlo samples. Using
Jensen’s inequality, we can show that the OCM objective defined in(11) provides an upper bound for
the grounded objective
Eq(˜x)
hϕ(˜x) − Eq(v)[v ⊙ H(˜x)v]
2
2 = Eq(˜x)
Eq(v)[hϕ(˜x) − v ⊙ H(˜x)v]
2
2
≤ Eq(˜x)Eq(v) ∥hϕ(˜x) − v ⊙ H(˜x)v∥2
2
= Locm(ϕ).
Thus, minimizing the OCM objective also minimizes the grounded objective, leading to a more
accurate approximation of the diagonal Hessian. We then show that these two objectives are equivalent
when attaining their optimal. To see this, we can expand the OCM objective
Locm(ϕ) = Ep(v)q(˜x)||hϕ(˜x) − v ⊙ H(˜x)v||2
2
= Eq(˜x)||hϕ(˜x)||2
2 − 2Ep(v)q(˜x)[hϕ(˜x)T (v ⊙ H(˜x)v)] + c
= Eq(˜x)||hϕ(˜x)||2
2 − 2Eq(˜x)[hϕ(˜x)T diag(H(˜x))] + c
= Eq(˜x)||hϕ(˜x) − diag(H(˜x))||2
2 + c′,
where c, c′ are constants w.r.t. the parameter ϕ, and line 2 to line 3 follows from the fact that
Ep(v)[v ⊙ H(x)v] = diag(H(x)). Therefore, Locm(ϕ) attains optimal when hϕ(˜x) = diag(H(˜x))
for all ˜x ∼ q(˜x).
A.3 C ONNECTION TO SN-DDPM
In this section, we showcase the connection between OCM-DDPM and SN-DDPM through the
second-order Tweedie’s formula. Before delving into it, we present the following lemmas, which are
essential for our derivation.
Lemma 1 (First order Tweedie’s formula (Efron, 2011)). Let q(˜x|x) = N (˜x|√αx, βI), we have the
mean of the inverse density equals
Eq(x|˜x)[x] = 1√α(˜x + β∇˜x log q(˜x)). (25)
Lemma 2 (Second order Tweedie’s formula). Let q(˜x|x) = N (˜x|√αx, βI), we have the second
moment of the inverse density equals
Eq(x|˜x)[xxT ] = 1
α
 
˜x˜xT + βs1(˜x)˜xT + β˜xs1(˜x)T + β2s2(˜x) + β2s1(˜x)s1(˜x)T + βI

, (26)
where s1(˜x) ≡ ∇ ˜x log q(˜x) and s2(˜x) ≡ ∇2
˜x log q(˜x).
Proof. The proof follows that in (Meng et al., 2021, Appendix B) with the generalization to the scaled
Gaussian convolutions. Specifically, we first reparametrizedq(˜x|x) as a exponential distribution
q(˜x|η) = eηT ˜x−ψ(η)q0(˜x), (27)
where η =
√α
β x, q0(˜x) ∝ e− 1
2β ˜xT ˜x, and ψ(η) denotes the partition function. By applying the Bayes
rule q(η|˜x) = q(˜x|η)p(η)
q(˜x) , we have the corresponding posterior
q(η|˜x) = eηT ˜x−ψ(η)−λ(˜x)p(η), (28)
16


=== PAGE 17 ===
Published as a conference paper at ICLR 2025
where λ(˜x) ≡ log q(˜x) − log q0(˜x). Since
R
q(η|˜x)dη = 1, by taking the derivative w.r.t. ˜x on both
sides, we have Z
(η − ∇˜xλ(˜x))T q(η|˜x) = 0, (29)
which implies that E[η|˜x] = ∇˜xλ(˜x). Taking the derivative w.r.t. ˜x on both sides again, we haveZ
η (η − ∇˜xλ(˜x))T q(η|˜x) = ∇2
˜xλ(˜x), (30)
which implies that E[ηη T |˜x] = ∇2
˜xλ(˜x) + ∇˜xλ(˜x)∇T
˜x λ(˜x). By substituting η =
√α
β x, ∇˜xλ(˜x) =
s1(˜x) + 1
β ˜x, and ∇2
˜xλ(˜x) = s2(˜x) + 1
β I, we get the result as desired.
Lemma 3 (Convert the covariance of q(˜x|x) to the hessian of q(˜x)). Let q(˜x|x) = N (˜x|√αx, βI),
we have the covariance of the inverse density equals
Covq(x|˜x)[x] = β
α(I + β∇2
˜x log q(˜x)). (31)
Proof. Let s1(˜x) ≡ ∇ ˜x log q(˜x) and s2(˜x) ≡ ∇2
˜x log q(˜x). We have
Covq(x|˜x)[x] = β2
α Covq(x|˜x)
 ˜x − √αx
β

= β2
α
 
Eq(x|˜x)
 ˜x−√αx
β
 ˜x−√αx
β
T
− Eq(x|˜x)
 ˜x−√αx
β

Eq(x|˜x)
 ˜x−√αx
β
T!
= β2
α
 1
β2 Eq(x|˜x)
 
˜x − √αx
 
˜x − √αx
T
− s1(˜x)s1(˜x)T

= β2
α
 1
β2
 
˜x˜xT − 2˜x(˜x + βs1(˜x))T + αEq(x|˜x)[xxT ]

− s1(˜x)s1(˜x)T

= β2
α
 1
β2
 
β2s2(˜x) + β2s1(˜x)s1(˜x)T + βI

− s1(˜x)s1(˜x)T

= β2
α
 1
β I + s2(˜x)

≡ β
α
 
I + β∇2
˜x log q(˜x)

,
where line 2 to line 3 follows Lemma 1 and line 4 to line 5 follows Lemma 2.
It is noteworthy that line 3 in the proof of Lemma 3 also showcases the connection between the
covariance of q(˜x|x) and the score of q(˜x):
Covq(x|˜x)[x] = β2
α
 1
β Eq(x|˜x)[ϵϵT ] − ∇˜x log q(˜x)∇˜x log q(˜x)T

, (32)
where ϵ = (˜x − √αx)/√β. Now we can apply Equations (31) and (32) to establish the connection
between OCM-DDPM and SN-DDPM, as demonstrated in the following theorem.
Theorem 3 (Connection between OCM-DDPM and SN-DDPM). Suppose q(x0:T ) is defined as Equa-
tion (1), and the pre-trained score function is well-learned, i.e., ∇xt log pθ(xt) = ∇xt log q(xt), ∀xt.
Let hϕ, gψ be parameterized neural networks. OCM-DDPM learns hϕ by minimizing the objective
LOCM(ϕ) as in Equation (13), and SN-DDPM learns gψ by minimizing the objective LSN(ψ) as in
Equation (22). Then in optimal training with ϕ∗ = argmin LOCM(ϕ) and ψ∗ = argmin LSN(ψ), we
have the optimal diagonal covariance of OCM-DDPM Σt−1(xt; ϕ∗) = (1−αt)2hϕ∗(xt)+(1−αt)I
αt
and
that of SN-DDPM Σt−1(xt; ψ∗) = 1−¯αt−1
1−¯αt
βt + β2
t
αt

gψ∗(xt)
1−¯αt
−∇xt logpθ(xt)2

are identical.
Proof. In optimal training, we know that hϕ∗(xt) = diag( ∇2
xt log q(xt)) and gψ∗(xt) =
Eq(x0|xt)[ϵ2
t ]. Bao et al. (2022b) show that the covariance of the denoising density q(xt−1|xt)
has a closed form (see Lemma 13 in Bao et al. (2022b))
Covq(xt−1|xt)[xt−1] = λ2
t I + γ2
t Covq(x0|xt)[x0], (33)
17


=== PAGE 18 ===
Published as a conference paper at ICLR 2025
where λ2
t = 1−¯αt−1
1−¯αt
βt and γt = √¯αt−1 −
p
1 − ¯αt−1 − λ2
t
q
¯αt
1−¯αt
= √¯αt−1
βt
1−¯αt
. Since
q(xt|x0) = N (√¯αtx0, (1 − ¯αt)I), applying Lemma 3 gives
diag(Covq(x0|xt)[x0]) = (1 − ¯αt)
¯αt
(I + (1 − ¯αt)hϕ∗(xt)).
Substituting it into Equation (33), we have
diag(Covq(xt−1|xt)[xt−1]) = (1 − αt)2hϕ∗(xt) + (1 − αt)I
αt
= Σt−1(xt; ϕ∗)
as desired. Alternatively, applying Equation (32), we have
diag(Covq(x0|xt)[x0]) = (1 − ¯αt)2
¯αt
 gψ∗(xt)
1 − ¯αt
− ∇xt logpθ(xt)2

.
Substituting it into Equation (33) gives
diag(Covq(xt−1|xt)[xt−1]) = 1 − ¯αt−1
1 − ¯αt
βt + β2
t
αt
gψ∗(xt)
1− ¯αt
−∇xt logpθ(xt)2

= Σt−1(xt; ψ∗).
Therefore, diag(Covq(xt−1|xt)[xt−1]) = Σt−1(xt; ϕ∗) = Σt−1(xt; ψ∗) as desired.
Remark. Theorem 3 establishes the connection between OCM-DDPM and SN-DDPM using the
second-order Tweedie’s formula. This connection allows OCM-DDPM to utilize the same covariance
clipping trick as in SN-DDPM (see Appendix B.2 for details). Additionally, as highlighted in Bao
et al. (2022a), SN-DDPM suffers from error amplification in the quadratic term, a limitation not
present in OCM-DDPM. As shown in Figure 4 and Table 3, OCM-DDPM demonstrates superior
covariance estimation accuracy and likelihood performance compared to SN-DDPM, underscoring
the advantages of the proposed optimal covariance matching objective.
B D ETAILS OF EXPERIMENTS
B.1 D ETAILS OF MODEL ARCHITECTURES
In this section, we describe our model architectures in detail. Following the approach in Bao et al.
(2022a), our model comprises a pretrained score neural network with fixed parameters, along with a
trainable diagonal Hessian prediction network built on top of it.
Table 6: Source of pretrained score prediction networks
used in our experiments.
PROVIDED BY
CIFAR10 (LS) B AO ET AL . (2022 B)
CIFAR10 (CS) B AO ET AL . (2022 B)
CELEB A 64X64 S ONG ET AL . (2020)
IMAGE NET 64X64 N ICHOL & DHARIWAL (2021)
LSUN B EDROOM HO ET AL . (2020)
IMAGE NET 256 X256 P EEBLES & XIE (2023)
Details of Pretrained Score Prediction
Networks. Table 6 lists the pretrained
neural networks utilized in our experi-
ments. These models parameterize the
noise prediction ϵθ(x), allowing us to de-
rive the score prediction as sθ(xt) =
∇x log pθ(xt) = − ϵθ(xt)√1−¯αt
, following the
forward process defined in Equation (1). It
is important to note that the pretrained net-
works for ImageNet 64x64 and 256x256
include both noise prediction networks and covariance networks. In our model architectures, we
utilize only the noise prediction networks.
Table 7: Architecture details of our models.
NN1 NN2
CIFAR10 (LS) Conv Conv
CIFAR10 (CS) Conv Conv
CELEB A 64X64 Conv Conv
IMAGE NET 64X64 Conv Res +Conv
LSUN B EDROOM Conv Res +Conv
IMAGE NET 256 X256 AdaLN+Linear AdaLN+Linear
Details of Diagonal Hessian Prediction
Networks. For fair comparisons, we fol-
low the parameterization as per Bao et al.
(2022a) for all models excluding the one
on ImageNet 256x256, which was not ex-
plored in their paper. The architecture de-
tails of NN1 and NN2 are provided in Ta-
ble 7, where Conv denotes the convolu-
tional layer, Res denotes the residual block
18


=== PAGE 19 ===
Published as a conference paper at ICLR 2025
Table 8: The number of parameters and the averaged time (ms) to run a model function evaluation.
All are evaluated with a batch size of 64 on one A100-80GB GPU.
SCORE PREDICTION
NETWORK
SCORE & SN PREDICTION
NETWORKS
SCORE & DIAGONAL HESSIAN
PREDICTION NETWORKS
CIFAR10 (LS) 52.54 M / 44.37 MS 52.55 M / 44.61 MS (+0.5%) 52.55 M / 44.51 MS (+0.3%)
CIFAR10 (CS) 52.54 M / 45.13 MS 52.55 M / 45.24 MS (+0.2%) 52.55 M / 45.23 MS (+0.2%)
CELEBA 64X64 78.70 M / 67.88 MS 78.71 M / 69.15 MS (+1.9%) 78.71 M / 68.89 MS (+1.5%)
IMAGENET 64X64 121.06 M / 106.58 MS 121.49 M / 112.53 MS (+5.6%) 121.49 M / 112.23 MS (+5.3%)
LSUN BEDROOM 113.67 M / 692.58 MS114.04 M / 771.55 MS (+11.4%) 114.04 M / 770.73 MS (+11.2%)
IMAGENET 256X256 675.13 M / 22.84 MS MISSING MODEL 677.80 M / 23.01 MS (+0.7%)
(He et al., 2016), AdaLN denotes the adaptive layer norm block (Peebles & Xie, 2023), and Linear
denotes the linear layer.
Cost of Memory and Inference Time. In Table 8, we present the number of parameters and
inference time for various models. It is evident that the additional memory cost of the diagonal
Hessian prediction network is minimal compared to the original diffusion models, which only include
the score prediction. Regarding the extra inference cost, it is negligible for CIFAR10, CelebA 64x64,
and ImageNet 256x256. The additional time is at most 5.3% on ImageNet 64x64, and 11.2% on
LSUN Bedroom, but this is offset by the benefit of requiring fewer sampling steps to achieve an FID
around 6, as shown in Table 4. Notably, although the model size on ImageNet 256x256 is the largest,
it has the shortest inference time because DiT learns the diffusion model in the latent space.
Details of Table 4. In Table 4, the FID results of baselines are taken from Bao et al. (2022a).
Notably, the time cost for a single neural function evaluation is identical across DDPM, DDIM,
and Analytic-DPM, all of which use a fixed isotropic covariance in the backward Markov process.
The ratio of the time cost to the baselines is based on Table 8. For the FID results of our model,
OCM-DPM, we report performance using the DDPM forward process for LSUN Bedroom and the
DDIM forward process for the other datasets.
B.2 D ETAILS OF TRAINING , INFERENCE , AND EVALUATION
Our training and inference recipes largely follow those outlined in Bao et al. (2022a); Peebles & Xie
(2023). Below, we provide a detailed description.
Training Details. We use the AdamW optimizer (Loshchilov, 2017) with a learning rate of 0.0001
and train for 500K iterations across all datasets. The batch sizes are set to 64 for LSUN Bedroom,
128 for CIFAR10, CelebA 64x64, and ImageNet 64x64, and 256 for ImageNet 256x256. During
training, checkpoints are saved every 10K iterations, and we select the checkpoint with the best FID
on 2048 samples generated with full sampling steps2. We train our models using one A100-80G GPU
for CIFAR10, CelebA 64x64, and ImageNet 64x64; four A100-80G GPUs for LSUN Bedroom; and
eight A100-80G GPUs for ImageNet 256x256.
Sampling Details. As per Bao et al. (2022b), covariance clipping is crucial to the performance
in diffusion models with unfixed variance in the backward Markovian. Leveraging the connection
between OCM-DPM and SN-DPM established in Theorem 3, we can apply the same clipping
strategies outlined in Bao et al. (2022a;b). Specifically, we only display the mean of p(x0|x1) at the
last sampling and clip the covariance Σ1(x2) of p(x1|x2) such that ∥Σ1(x2)∥∞E|ϵ| ≤ 2
255 y, where
∥·∥∞ denotes the infinity norm andϵ is the standard Gaussian noise. Following Bao et al. (2022b), we
use y = 2 on CIFAR10 (LS) and CelebA 64x64 under the DDPM forward process, and use y = 1 for
other cases. For all skip-step sampling methods, we employ the even trajectory (Nichol & Dhariwal,
2021) for selecting the subset of sampling steps.
Evaluation Details. The performance is evaluated on the exponential moving average (EMA) model
with a rate of 0.9999. For computing the negative log-likelihood, we follow Ho et al. (2020); Bao
et al. (2022a) by discretizing the last sampling step p(x0|x1) to obtain the likelihood of discrete
image data and report the upper bound of the negative log-likelihood on the entire test set. The
FID score is computed on 50K generated samples. Following Nichol & Dhariwal (2021); Bao et al.
2We use 2048 samples instead of 1000 to ensure that the covariance of the Inception features has full rank.
19


=== PAGE 20 ===
Published as a conference paper at ICLR 2025
Table 9: The corresponding mean and standard deviation of NLL and FID reported in Tables 3 and 5.
Note that the standard deviation is reported under the scale of percentage (%).
CIFAR10 (LS) CIFAR10 (CS)
# TIMESTEPS K 10 25 50 100 200 1000 10 25 50 100 200 1000
MEAN -DDPM (NLL) 5.33 4.63 4.35 3.97 3.78 3.57 4.99 4.34 3.99 3.76 3.59 3.41
STD-DDPM (NLL) % 0.04 0.03 0.03 0.04 0.04 0.01 0.02 0.01 0.03 0.03 0.02 0.01
MEAN -DDPM (FID) 25.07 9.30 5.90 4.37 3.54 4.00 14.46 5.49 4.09 3.83 3.81 4.22
STD-DDPM (FID) % 9.05 7.54 3.48 2.77 4.25 2.30 10.40 7.74 2.66 1.32 5.26 3.08
MEAN -DDIM (FID) 10.61 4.31 3.48 3.26 3.25 3.75 16.85 6.70 4.73 4.33 4.56 4.59
STD-DDIM (FID) % 5.17 3.55 3.19 1.87 3.97 0.99 13.22 4.23 2.28 2.94 1.61 5.30
CELEB A 64 X64 I MAGE NET 64X64
# TIMESTEPS K 10 25 50 100 200 1000 25 50 100 200 400 4000
MEAN -DDPM (NLL) 4.69 3.86 3.43 3.13 2.90 2.66 4.45 4.15 3.93 3.79 3.70 3.59
STD-DDPM (NLL) % 0.01 0.01 0.01 0.00 0.01 0.00 0.01 0.00 0.01 0.00 0.01 0.01
MEAN -DDPM (FID) 21.58 12.65 9.24 6.96 6.00 5.02 28.01 20.85 18.01 16.76 16.35 16.33
STD-DDPM (FID) % 2.24 7.65 2.29 2.32 5.50 1.37 0.77 4.66 6.82 3.28 3.04 3.21
MEAN -DDIM (FID) 10.36 5.69 4.37 3.50 3.11 3.03 28.34 19.68 17.84 17.41 17.36 18.91
STD-DDIM (FID) % 7.40 4.04 5.12 4.88 6.46 1.55 6.59 3.77 9.15 4.08 7.88 5.26
(a) Cov Error with True Score
 (b) Cov Error with Learned Score
 (c) DDPM MMD v.s. Steps
 (d) DDIM MMD v.s. Steps
Figure 4: Comparisons of different covariance estimation methods based on estimation error and
sample generation quality. Figures (a) and (b) show the mean square error of the estimated diagonal
covariance under the assumptions: (a) access to the true score, and (b) learned score of the data
distribution at various noise levels. Figures (c) and (d) present the MMD evaluation against the total
sampling steps in the DDPM (c) and DDIM (d) settings. We can find the proposed OCM method can
achieve the lowest estimation error and consistently outperform other baseline methods when fewer
generation steps are applied.
(2022a); Peebles & Xie (2023), the reference distribution statistics for FID are calculated using
the full training set for CIFAR10 and ImageNet, and 50K training samples for CelebA and LSUN
Bedroom. Performance results are reported using the same random seed 3 as in Bao et al. (2022a).
Additionally, the variance across different random seeds is provided in Appendix C.4.
C A DDITIONAL EXPERIMENTAL RESULTS
In this section, we present additional experimental results for comparison. First, we present an
additional toy experiment in Appendix C.1 and likelihood comparison between I-DDPM, NPR-
DDPM, and OCM-DDPM in Appendix C.2 and present results using different numbers of Rademacher
samples in Appendix C.3. We also analyze the performance variance in Appendix C.4. Additionally,
we include further experimental results on latent diffusion models in Appendix C.5 and conduct
qualitative studies by showcasing samples generated by our models in Appendix C.6.
C.1 A DDITIONAL TOY DEMONSTRATION
To further verify the effectiveness of our method, we include an additional toy example. In this
case, we consider another two-dimensional mixture of nine Gaussians (MoG) with means located at
3We use the official code fromhttps://github.com/baofff/Extended-Analytic-DPM.
20


=== PAGE 21 ===
Published as a conference paper at ICLR 2025
{−3, 0, 3} ⊗ {−3, 0, 3} and a standard deviation of σ = 0.1. To assess different approaches, we first
learn the covariance using the true scores. Specifically, we train the covariance networks for these
methods over 50,000 iterations with a learning rate of 0.001 using an Adam optimizer (Kingma &
Ba, 2014). Figure 4a shows the L2 error of the estimated diagonal covariance Σt−1(xt), and our
method, OCM-DDPM, consistently achieves the lowest error compared to the other methods. In
practice, the true score is not accessible. Therefore, we also conduct a comparison using a score
function learned with DSM. We then apply different covariance estimation methods under the same
settings. In Figure 4b, we plot the same L2 error for the learned score setting and find that our method
achieves the lowest error for all t values.
We further use the learned score and the covariance estimated by different methods to conduct DDPM
and DDIM sampling for this MoG problem. Figures 4c and 4d presents the MMD comparison
across various methods. It shows that our method outperforms the baselines when the total time
step is small, demonstrating the importance of accurate covariance estimation in the context of
diffusion acceleration. We also include two methods utilizing the true diagonal and full covariance
as benchmarks, representing the best achievable performance. Notably, these two methods exhibit
similar performance because the MoG used in this setting has symmetric components, in which the
covariance is dominated by the diagonal entries.
C.2 A DDITIONAL LIKELIHOOD COMPARISON
Table 10: Upper bound on the negative log-likelihood
(bits/dim) on the ImageNet 64x64 dataset.
# TIMESTEPS K 25 50 100 200 400 4000
I-DDPM 18.91 8.46 5.27 4.24 3.86 3.57
NPR-DDPM 4.66 4.22 3.96 3.80 3.71 3.60
SN-DDPM 4.56 4.18 3.95 3.80 3.71 3.63
OCM-DDPM 4.45 4.15 3.93 3.79 3.70 3.59
Here, we provide additional likelihood
comparisons against I-DDPM (Nichol &
Dhariwal, 2021) on the ImageNet 64x64
dataset. As discussed in Section 4, I-
DDPM parameterizes the diagonal covari-
ance by interpolating between β and and
learns the covariance by maximizing the
variational lower bound. For ease of com-
parison, we also include the performance
of NPR-DDPM and SN-DDOM, which em-
ploy an alternative MSE loss as detailed in
Equation (22) to learn the covariance. Table 10 presents the results, showing that OCM-DDPM
achieves the best likelihood with fewer sampling steps while maintaining performance comparable
to I-DDPM when using full sampling steps. This highlights the superiority of the proposed optimal
covariance matching objective in learning a diffusion model for data density estimation.
C.3 I MPACT OF THE NUMBERS OF RADEMACHER SAMPLES
Table 11: Results of OCM-DDPM on CIFAR10 (CS)
with varying numbers of Rademacher Samples.
FID ↓ NLL (%) ↑
K = 10 25 50 100 10 25 50 100
M = 1 14.32 5.54 4.10 3.84 4.99 4.34 3.99 3.76
M = 3 14.18 5.51 4.11 3.82 4.99 4.34 3.99 3.76
M = 5 14.17 5.51 4.11 3.82 4.99 4.34 3.99 3.76
M = 10 14.16 5.51 4.11 3.82 4.99 4.34 3.99 3.76
In this section, we include an ablation study
examining the effect of varying the number
of Rademacher samples M. Specifically,
we conduct experiments on CIFAR10 (CS)
using the proposed OCM-DDPM method
and evaluate performance based on FID
and NLL. The empirical results, presented
in Table 11, indicate that a larger M (e.g.,
M = 3 ) can give a small improvement
in FID. This improvement is likely due to
the reduced gradient estimation variance
during training with a larger M. However,
in most cases, different values of M yield consistent performance. This is practically desirable as, in
practice, setting M = 1 allows for efficient training while maintaining strong performance
C.4 M EAN AND VARIANCE OF PERFORMANCE
In Tables 3 and 5, we evaluate our models using the same random seed as Bao et al. (2022a). To
minimize the impact of randomness, we report the mean and standard deviation in Table 9 by repeating
the evaluation three times with different seeds.
21


=== PAGE 22 ===
Published as a conference paper at ICLR 2025
Table 12: Results with varying CFG coefficients using 10 sampling steps on ImageNet 256x256.
FID ↓ RECALL (%) ↑
CFG = 1.5 1.75 2.0 3.0 4.0 1.5 1.75 2.0 3.0 4.0
DDPM, ˜β 30.41 19.26 13.53 11.52 14.76 39.74 35.29 32.08 24.93 19.91
DDPM, β 210.28 182.78 158.16 89.86 58.16 16.98 22.28 25.13 23.07 19.48
I-DDPM 44.96 20.71 13.01 9.76 13.75 50.32 43.75 41.08 31.77 25.49
OCM-DDPM 30.55 17.57 11.12 9.52 13.74 49.39 45.87 42.33 32.13 24.96
DDIM 9.41 6.54 6.12 10.49 14.18 49.15 45.82 41.15 29.27 23.16
OCM-DDIM 11.30 6.67 5.33 9.20 13.49 54.50 50.32 46.58 34.53 25.67
Table 13: Results with CFG=1.5 across different sampling steps on ImageNet 256x256.
FID ↓ RECALL (%) ↑
# TIMESTEPS K 10 25 50 100 200 250 10 25 50 100 200 250
DDPM, ˜β 30.41 4.79 3.55 3.09 3.05 2.97 39.74 49.02 51.79 53.54 54.21 54.34
DDPM, β 210.28 42.09 9.43 3.63 2.91 2.75 16.98 46.76 51.67 55.01 55.50 55.21
I-DDPM 44.96 9.01 3.70 2.48 2.25 2.74 50.32 54.78 56.45 57.88 58.76 54.69
OCM-DDPM 30.55 4.96 3.21 2.71 2.50 2.75 49.39 53.75 54.68 55.85 54.97 54.75
DDIM 9.41 2.70 2.33 2.25 2.23 2.20 49.15 56.41 57.00 57.83 57.81 57.99
OCM-DDIM 11.30 3.26 2.56 2.28 2.23 2.18 54.50 58.33 58.09 58.80 58.16 58.56
C.5 M ORE RESULTS ON LATENT DIFFUSION MODELS
We report the performance using 10 sampling steps with varying CFG coefficients in Table 12.
The results indicate that our methods perform best at CFG = 2 .0. While DDPM- ˜β and DDIM
show strong FID scores, their Recall is lower due to fixed variance, suggesting less diversity in
the generated samples. In Table 13, we further compare the performance across different sampling
steps at CFG = 1.5. The results again demonstrate that our methods, which estimate the optimal
covariance from the data, produce more diverse samples while maintaining comparable image quality.
C.6 G ENERATED SAMPLES
In this section, we conduct qualitative studies by showcasing the generated samples from our models
using different sampling steps K. The results are summarized as follows:
• In Figure 6, we visualize the generated samples using varying numbers of Monte Carlo
samples with the Rademacher estimator (refer to Table 1).
• In Figure 7, we visualize the training data and the generated samples using the minimum
number of sampling steps required to achieve an FID of approximately 6 (refer to Table 4).
• In Figures 8 to 11, we visualize the generated samples of our models using different number
of sampling steps on CIFAR10 (LS), CIFAR10 (CS), CelebA 64x64, and ImageNet 64x64,
respectively (refer to Table 5).
• In Figure 12, we visualize the generated samples on ImageNet 256x256, using 10 sampling
steps with different CFG coefficients. (refer to Figure 3 and Table 12).
• In Figure 13, we visualize the generated samples on ImageNet 256x256, using varying
number of sampling steps with CFG set to 1.5 (refer to Table 13).
22


=== PAGE 23 ===
Published as a conference paper at ICLR 2025
Data
 M = 1
 M = 10
 M = 100
 M = 1000
Figure 5: Diagonal covariance estimation visualisation with different Rademacher sample numbers.
(a) OCM-DDPM (M =5 )
 (b) OCM-DDPM (M =10 )
 (c) OCM-DDPM (M =15 )
 (d) OCM-DDPM (M =20 )
Figure 6: Generated samples with 10 sampling steps using ifferent Rademacher sample numbers on
CIFAR10 (CS) (ref: Table 1).
(a) CIFAR10 Data
 (b) CelebA 64x64 Data
 (c) LSUN Bedroom Data
 (d) ImageNet 256x256 Data
(e) CIFAR10 (K =16 )
 (f) CelebA 64x64 (K =17 )
 (g) LSUN Bedroom (K =90 )
 (h) ImageNet 256x256 (K =10 )
Figure 7: The training data and generated samples of OCM-DPM with minimum steps to achieve an
FID around 6. (ref: Table 4)
23


=== PAGE 24 ===
Published as a conference paper at ICLR 2025
(a) OCM-DDPM (K = 10)
 (b) OCM-DDPM (K = 25)
 (c) OCM-DDPM (K = 50)
 (d) OCM-DDPM (K = 100)
(e) OCM-DDIM (K = 10)
 (f) OCM-DDIM (K = 25)
 (g) OCM-DDIM (K = 50)
 (h) OCM-DDIM (K = 100)
Figure 8: Generated samples with different sampling steps on CIFAR10 (LS) (ref: Table 5).
(a) OCM-DDPM (K = 10)
 (b) OCM-DDPM (K = 25)
 (c) OCM-DDPM (K = 50)
 (d) OCM-DDPM (K = 100)
(e) OCM-DDIM (K = 10)
 (f) OCM-DDIM (K = 25)
 (g) OCM-DDIM (K = 50)
 (h) OCM-DDIM (K = 100)
Figure 9: Generated samples with different sampling steps on CIFAR10 (CS) (ref Table 5).
24


=== PAGE 25 ===
Published as a conference paper at ICLR 2025
(a) OCM-DDPM (K = 10)
 (b) OCM-DDPM (K = 25)
 (c) OCM-DDPM (K = 50)
 (d) OCM-DDPM (K = 100)
(e) OCM-DDIM (K = 10)
 (f) OCM-DDIM (K = 25)
 (g) OCM-DDIM (K = 50)
 (h) OCM-DDIM (K = 100)
Figure 10: Generated samples with different sampling steps on CelebA 64x64 (ref: Table 5).
(a) OCM-DDPM (K = 10)
 (b) OCM-DDPM (K = 25)
 (c) OCM-DDPM (K = 50)
 (d) OCM-DDPM (K = 100)
(e) OCM-DDIM (K = 10)
 (f) OCM-DDIM (K = 25)
 (g) OCM-DDIM (K = 50)
 (h) OCM-DDIM (K = 100)
Figure 11: Generated samples with different sampling steps on ImageNet 64x64 (ref: Table 5).
25


=== PAGE 26 ===
Published as a conference paper at ICLR 2025
(a) OCM-DDPM (CFG=1 .5)
 (b) OCM-DDPM (CFG=1 .75)
 (c) OCM-DDPM (CFG=2 .0)
 (d) OCM-DDPM (CFG=3 .0)
(e) OCM-DDIM (CFG=1 .5)
 (f) OCM-DDIM (CFG=1 .75)
 (g) OCM-DDIM (CFG=2 .0)
 (h) OCM-DDIM (CFG=3 .0)
Figure 12: Generated samples with 10 sampling steps using different CFG coefficients on ImageNet
256x256 (ref: Table 12)
(a) OCM-DDPM (K = 25)
 (b) OCM-DDPM (K = 50)
 (c) OCM-DDPM (K = 100)
 (d) OCM-DDPM (K = 200)
(e) OCM-DDIM (K = 25)
 (f) OCM-DDIM (K = 50)
 (g) OCM-DDIM (K = 100)
 (h) OCM-DDIM (K = 200)
Figure 13: Generated samples with CFG=1.5 using different sampling steps on ImageNet 256x256
(ref: Table 13)
26